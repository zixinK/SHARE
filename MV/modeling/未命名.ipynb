{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4262c88f-b866-4a47-97a2-9e0648889fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c05ddf90-9785-43d1-9e61-381b3d2129c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04f8d969-7ad5-4aac-a444-9fb9017a8207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/mist/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e87d6f4be7884bcca9c5f4e27c902239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Download and cache the dataset\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cf8648e-940b-4828-bd07-6056f95b2042",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_dataset = raw_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11369e98-0c36-42f1-8d2f-8446f23f527c",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a51b029-490e-4490-9c96-74242d673012",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenized_sentences_1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"])\n",
    "tokenized_sentences_2 = tokenizer(raw_datasets[\"train\"][\"sentence2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c36f091-45f3-417d-b9cf-d8a4a9299db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/mist/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-6fd6978f4d0e1f7f.arrow\n",
      "Loading cached processed dataset at /home/mist/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-e21445cecc3a2248.arrow\n",
      "Loading cached processed dataset at /home/mist/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-74cc38742f918009.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    \"\"\"\n",
    "    This function takes a dictionary (like the items of our dataset) and returns a new dictionary \n",
    "    with the keys input_ids, attention_mask, and token_type_ids. \n",
    "        - Note that it also works if the example dictionary contains several samples \n",
    "          (each key as a list of sentences) since the tokenizer works on lists \n",
    "          of pairs of sentences, as seen before. \n",
    "    \n",
    "    This will allow us to use the option batched=True in our call to map(), \n",
    "    which will greatly speed up the tokenization. \n",
    "        - The tokenizer is backed by a tokenizer written in Rust from the \n",
    "          🤗 Tokenizers library. \n",
    "        - This tokenizer can be very fast, but only if we give it lots of \n",
    "          inputs at once.\n",
    "    \n",
    "    Note that we’ve left the padding argument out in our tokenization function for now. \n",
    "        - This is because padding all the samples to the maximum length is not efficient\n",
    "        - It’s better to pad the samples when we’re building a batch, as then we only need to \n",
    "          pad to the maximum length in that batch, and not the maximum length in the entire dataset. \n",
    "        - This can save a lot of time and processing power when the inputs have variable lengths.\n",
    "    \n",
    "    Args:\n",
    "        example (Dict): A dictionary containing the items of the dataset.\n",
    "    \n",
    "    Returns:\n",
    "        A new dictionary with the keys input_ids, attention_mask, and token_type_ids.\n",
    "    \n",
    "    \"\"\"\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "# Here is how we apply the tokenization function on all our datasets at once. \n",
    "# \n",
    "# We’re using batched=True in our call to map so the function is applied to \n",
    "# multiple elements of our dataset at once, and not on each element separately. \n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dc9246f-e1de-4d40-a051-09522cd28318",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Remove the columns corresponding to values the model does not expect\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
    "\n",
    "# Rename the column label to labels\n",
    "#      - Because the model expects the argument to be named `labels`\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "\n",
    "# Set the format of the datasets so they return PyTorch tensors\n",
    "tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "780ac724-1ae9-4c36-b88e-7b91ebfd7059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "l_len = len(tokenized_datasets[\"train\"])\n",
    "random.seed(1234)\n",
    "line_r = [random.randint(0,l_len-1) for _ in range(int(l_len*0.01))]\n",
    "tokenized_datasets[\"train\"] = [tokenized_datasets[\"train\"][i] for i in line_r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8de9e827-816d-4092-942f-4a1d981b5052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "l_len = len(tokenized_datasets[\"validation\"])\n",
    "random.seed(1234)\n",
    "line_r = [random.randint(0,l_len-1) for _ in range(int(l_len*0.01))]\n",
    "tokenized_datasets[\"validation\"] = [tokenized_datasets[\"validation\"][i] for i in line_r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "560abc3a-13ef-4c2d-8a7d-1552eb2fba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73bf7b87-dd3f-4399-871f-fc2dc4be4153",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], shuffle=True, batch_size=1, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], batch_size=1, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1dd60e7-f3b9-4b30-a3d7-c2fa9170a70f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorWithPadding(tokenizer=PreTrainedTokenizerFast(name_or_path='bert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}), padding=True, max_length=None, pad_to_multiple_of=None, return_tensors='tf')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c586ea9-3547-43d7-b49e-c3356722e5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tlabels          --> (1,)\n",
      "\tinput_ids       --> (1, 58)\n",
      "\ttoken_type_ids  --> (1, 58)\n",
      "\tattention_mask  --> (1, 58)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-14 15:41:50.374502: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-14 15:41:52.336235: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30527 MB memory:  -> device: 0, name: Tesla PG500-216, pci bus id: 0000:83:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    for k,v in batch.items(): print(f\"\\t{k:<15} --> {v.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "299de1a3-6d42-4921-9a01-d25c80b1534c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'labels': <tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, 'input_ids': <tf.Tensor: shape=(1, 58), dtype=int32, numpy=\n",
       "array([[  101,  1996,  6745,  5008,  5799,  2000,  1996, 11867,  9910,\n",
       "         2001,  1037,  2281,  2340,  5008,  2012,  5226,  2430,  4732,\n",
       "         2082,  1999, 15578,  5753,  1010,  2055,  1017, 22287,  2013,\n",
       "         1996, 10846,  1012,   102,  2178,  5008,  5799,  2000,  1996,\n",
       "        11867,  9910,  4158, 13292,  1012,  2340,  2012,  5226,  2430,\n",
       "         4732,  1999, 15578,  5753,  1010,  2055,  2048,  2661,  2013,\n",
       "         1996, 10846,  1012,   102]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 58), dtype=int32, numpy=\n",
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 58), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)>}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "380fe21d-db82-492e-98e4-7e06a2c15333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/mist/My Code/modeling'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37850a3f-b789-4992-8239-5507bed82518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "from typing import Optional\n",
    "from sklearn import metrics\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import tensorflow as tf\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "from transformers.file_utils import ModelOutput\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    BertModel,\n",
    "    BertPreTrainedModel,\n",
    "    )\n",
    "\n",
    "from connected_shapley import explain_shapley, construct_positions_connectedshapley\n",
    "from continuity1 import  onehat_pre,continuity,rl_cd_ss\n",
    "from logical_fluency import newset\n",
    "\n",
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "   'bert-base-uncased',\n",
    "    num_labels=2,\n",
    "\n",
    ")\n",
    "\n",
    "def compute_metrics_fn(eval_prediction, tokenizer=None, return_text=False):\n",
    "    # full_logits = eval_prediction.predictions[0]\n",
    "    masked_logits = eval_prediction.predictions[0]\n",
    "    mask = eval_prediction.predictions[1]\n",
    "    labels = eval_prediction.predictions[2]\n",
    "    mask_labels = eval_prediction.predictions[3]\n",
    "    mask_labels = np.maximum(mask_labels, 0)\n",
    "\n",
    "    s_loss = eval_prediction.predictions[5]\n",
    "    cph_loss = eval_prediction.predictions[6]\n",
    "    cpt_loss = eval_prediction.predictions[7]\n",
    "    lf_loss = eval_prediction.predictions[8]\n",
    "\n",
    "    # full_preds = np.argmax(full_logits, axis=1)\n",
    "    masked_preds = np.argmax(masked_logits, axis=1)\n",
    "    is_masked = np.greater(mask, 0.5).astype(np.int)\n",
    "\n",
    "    results = {}\n",
    "    # results[\"full_acc\"] = metrics.accuracy_score(labels, full_preds)\n",
    "    results[\"masked_acc\"] = metrics.accuracy_score(labels, masked_preds)\n",
    "    results[\"mask_f1\"] = metrics.f1_score(mask_labels, is_masked, average=\"micro\", zero_division=1)\n",
    "    results[\"mask_recall\"] = metrics.recall_score(mask_labels, is_masked, average=\"micro\", zero_division=1)\n",
    "    results[\"mask_precision\"] = metrics.precision_score(mask_labels, is_masked, average=\"micro\", zero_division=1)\n",
    "\n",
    "    results[\"suffiency_loss\"] = s_loss.mean()\n",
    "    results[\"comprehensiveness_loss\"] = cph_loss.mean()\n",
    "    results[\"compactness_loss\"] = cpt_loss.mean()\n",
    "    results[\"logical_fluence_loss\"] = lf_loss.mean()\n",
    "\n",
    "    if return_text:\n",
    "        input_ids = eval_prediction.predictions[4]\n",
    "        examples = []\n",
    "        for i in range(len(input_ids)):\n",
    "            import pdb; pdb.set_trace()\n",
    "            row = tokenizer.convert_ids_to_tokens(input_ids[i])\n",
    "            original = []\n",
    "            masked = []\n",
    "            for j in range(len(row)):\n",
    "                if row[j] in [\"[CLS]\", \"[SEP]\", \"<pad>\"]:\n",
    "                    continue\n",
    "                original.append(row[j])\n",
    "                if is_masked[i][j]:\n",
    "                    masked.append(\"▁<mask>\")\n",
    "                else:\n",
    "                    masked.append(row[j])\n",
    "            original = tokenizer.convert_tokens_to_string(original)\n",
    "            masked = tokenizer.convert_tokens_to_string(masked)\n",
    "            combined = f\"{original} OLD: {masked}\"\n",
    "            examples.append(combined)\n",
    "        results[\"text\"] = examples\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "class MaskOutput(ModelOutput):\n",
    "    rationale: torch.FloatTensor\n",
    "    noise: torch.FloatTensor\n",
    "    r_ind: torch.FloatTensor\n",
    "\n",
    "\n",
    "\n",
    "class ClassifyOutput(ModelOutput):\n",
    "    logits: torch.FloatTensor\n",
    "    loss: torch.FloatTensor\n",
    "\n",
    "\n",
    "class TokenTaggingRationaleOutput(ModelOutput):\n",
    "    loss: torch.FloatTensor\n",
    "    # full_logits: torch.FloatTensor\n",
    "    masked_logits: torch.FloatTensor\n",
    "    mask: torch.FloatTensor\n",
    "    labels: Optional[torch.FloatTensor] = None\n",
    "    mask_labels: Optional[torch.FloatTensor] = None\n",
    "    # is_unsupervised: Optional[torch.FloatTensor] = None\n",
    "    input_ids: Optional[torch.LongTensor] = None\n",
    "    s_loss: torch.FloatTensor = None\n",
    "    cph_loss:torch.FloatTensor = None\n",
    "    cpt_loss:torch.FloatTensor = None\n",
    "    lf_loss:torch.FloatTensor = None\n",
    "\n",
    "\n",
    "class BertForTokenRationale(BertPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertForTokenRationale, self).__init__(config)\n",
    "        self.config = config\n",
    "        self.num_labels = 2\n",
    "        # self.nei_index = config.nei_index\n",
    "        self.mask_token_id = tokenizer.mask_token_id\n",
    "        self.bert = BertModel(config).to(device)\n",
    "        # self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        # # nn.Dropout: 为了防止或减轻过拟合而使用的函数\n",
    "        # self.masker = nn.Linear(config.hidden_size, 2)\n",
    "        # # 是用来设置网络中的全连接层的，而在全连接层中的输入与输出都是二维张量，一般形状为[batch_size, size]\n",
    "\n",
    "\n",
    "        # self.sufficiency_weight = config.sufficiency_weight # sufficiency 好像不需要weight\n",
    "        self.config.cph_weight = 0.3\n",
    "        self.config.cpt_weight = 0.2\n",
    "        self.config.lf_weight = 0.2\n",
    "        # self.batch_size = config.per_device_train_batch_size\n",
    "\n",
    "        # self.train_size=config.per_device_train_batch_size\n",
    "        # self.max_seq_length = config.max_seq_length\n",
    "\n",
    "        # shapley\n",
    "        # self.max_order = config.max_order\n",
    "        self.num_neighbors = 2\n",
    "        self.top_percentage = 0.2\n",
    "        self.con_count =3\n",
    "\n",
    "        self.cph_margin =0.05\n",
    "        self.lf_seed=1234\n",
    "        self.lf_num = 1\n",
    "\n",
    "\n",
    "        self.init_weights()\n",
    "        self.classifier = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english').to(device)\n",
    "        for p in self.classifier.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def mask(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        # token_type_ids=None,\n",
    "        # position_ids=None,\n",
    "        evidence_mask=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        # print(type(attention_mask))\n",
    "        # print(attention_mask)\n",
    "        d = tf.reduce_sum(attention_mask, 1).numpy().tolist()\n",
    "        # attention_mask.tolist().sum(axis=1)\n",
    "        rationale_weights = []\n",
    "        noise_weights = []\n",
    "        r_d = []\n",
    "        for i in range(len(input_ids)):\n",
    "            positions_dict, key_to_idx, positions, coefficients, unique_inverse = construct_positions_connectedshapley(int(d[i]),k=2)\n",
    "\n",
    "\n",
    "            mask_weights = torch.Tensor(positions).unsqueeze(0).to(device)\n",
    "            mask_ids = torch.Tensor(input_ids[i].numpy())[:d[i]].clone().fill_(self.mask_token_id).to(device)\n",
    "            input_id = torch.Tensor(input_ids[i].numpy())[:d[i]].to(device)\n",
    "            mix_inputs = (1-mask_weights) * mask_ids +  mask_weights * input_id\n",
    "            mix_inputs = mix_inputs.long().squeeze().to(device)\n",
    "            \n",
    "            \n",
    "            score = explain_shapley(self.bert,int(d[i]), self.num_neighbors, key_to_idx,mix_inputs, input_id.long(),\n",
    "                                    coefficients, unique_inverse)\n",
    "\n",
    "            # score = torch.tensor(np.array(score))\n",
    "            r_len = math.ceil(int(d[i])*self.top_percentage)\n",
    "            # score_ind_order = torch.sort(score).indices\n",
    "            # score_val_order = torch.sort(score).values\n",
    "\n",
    "            one_hot = [onehat_pre(score,r_len)]\n",
    "            ra_can = continuity(score, one_hot, self.con_count)\n",
    "            s_d_r = rl_cd_ss(ra_can,score)\n",
    "\n",
    "            max_sdr_ind = s_d_r.index(max(s_d_r))\n",
    "\n",
    "\n",
    "            score_ind_order_d = torch.sort(torch.Tensor(np.array(score)),descending=False).indices[:r_len]\n",
    "\n",
    "            nw = torch.Tensor([1 if i in score_ind_order_d else 0 for i in range(int(d[i]))]).to(device)\n",
    "            nw = pad_sequence([nw,input_id]).T[0].tolist()\n",
    "\n",
    "            noise_weights.append(nw)\n",
    "\n",
    "\n",
    "            rw = torch.tensor(ra_can[max_sdr_ind]).to(device)\n",
    "            rw = pad_sequence([rw,input_id]).T[0].tolist()\n",
    "            rationale_weights.append(rw)\n",
    "            r_d.append([i for i, x in enumerate(ra_can[max_sdr_ind]) if x == 1])\n",
    "\n",
    "        rationale_weights = torch.Tensor(rationale_weights)\n",
    "        noise_weights = torch.Tensor(noise_weights)\n",
    "\n",
    "\n",
    "        rationale_weights = F.gumbel_softmax(rationale_weights, tau=0.1)\n",
    "        noise_weights = F.gumbel_softmax(noise_weights, tau=0.1)\n",
    "\n",
    "        return MaskOutput(\n",
    "            rationale=rationale_weights,\n",
    "            noise = noise_weights,\n",
    "            r_ind=r_d,\n",
    "\n",
    "        )\n",
    "\n",
    "    def classify(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        mask_weights=None,\n",
    "        attention_mask=None,\n",
    "        # token_type_ids=None,\n",
    "        # position_ids=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        def embed(ids):\n",
    "            embeds = self.classifier.get_input_embeddings()(ids)\n",
    "            return embeds\n",
    "\n",
    "        # Embed inputs.\n",
    "        input_ids = input_ids.long().to(device)\n",
    "        input_embeds = embed(input_ids)\n",
    "\n",
    "        # Targeted mask.\n",
    "        mask_ids = input_ids.clone().fill_(self.mask_token_id).to(device)\n",
    "        mask_embeds = embed(mask_ids).to(device)\n",
    "        mask_weights = mask_weights.unsqueeze(-1).to(device)\n",
    "\n",
    "        # Mix embeddings.\n",
    "        # 我们直接用0mask还是像这样用 additive embedding呢\n",
    "        mix_embeds = (1-mask_weights) * mask_embeds +  mask_weights * input_embeds\n",
    "        \n",
    "        # print(type(mix_embeds))\n",
    "        # print(type(attention_mask))\n",
    "        attention_mask = torch.Tensor(attention_mask.numpy()).long().to(device)\n",
    "        labels= torch.Tensor(labels.numpy()).long().to(device)\n",
    "        # Run model.\n",
    "        outputs = self.classifier(\n",
    "            inputs_embeds=mix_embeds,\n",
    "            # attention_mask=attention_mask,\n",
    "            # token_type_ids=token_type_ids,\n",
    "            # position_ids=position_ids,\n",
    "            return_dict=True\n",
    "        )\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return ClassifyOutput(\n",
    "            logits=logits,\n",
    "            loss=loss,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        # token_type_ids=None,\n",
    "        # position_ids=None,\n",
    "        evidence_mask=None,\n",
    "        labels=None,\n",
    "        mask_labels=None,\n",
    "        # is_unsupervised=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        # Encode the inputs and predict token-level masking scores.\n",
    "        mask_output = self.mask(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            # token_type_ids=token_type_ids,\n",
    "            # position_ids=position_ids,\n",
    "            evidence_mask=evidence_mask,\n",
    "            labels=None)\n",
    "        rationale_weights = mask_output.rationale\n",
    "        noise_weights = mask_output.noise\n",
    "        rationale_index = mask_output.r_ind\n",
    "\n",
    "\n",
    "        # Get the output with targeted masking.\n",
    "        masked_cls_output = self.classify(\n",
    "            input_ids=torch.Tensor(input_ids.numpy()),\n",
    "            mask_weights=rationale_weights,\n",
    "            attention_mask=attention_mask,\n",
    "            # token_type_ids=token_type_ids,\n",
    "            # position_ids=position_ids,\n",
    "            labels=labels)\n",
    "        # Suffiency\n",
    "        suffiency_loss = masked_cls_output.loss if labels is not None else 0\n",
    "\n",
    "        noise_cls_output = self.classify(\n",
    "            input_ids=torch.Tensor(input_ids.numpy()),\n",
    "            mask_weights=noise_weights,\n",
    "            attention_mask=attention_mask,\n",
    "            # token_type_ids=token_type_ids,\n",
    "            # position_ids=position_ids,\n",
    "            labels=labels)\n",
    "        # Noise\n",
    "        noise_loss = noise_cls_output.loss if labels is not None else 0\n",
    "\n",
    "        # Comprehensive\n",
    "        cph_loss = []\n",
    "        for i in range(len(suffiency_loss)):\n",
    "          cph_loss.append(max(suffiency_loss[i]-noise_loss[i]+self.cph_margin,0+1e-12))\n",
    "        cph_loss = torch.Tensor(cph_loss).to(device)\n",
    "\n",
    "        # Compute (soft) number of masked tokens\n",
    "        mask_w = mask_output.rationale.sum(dim=1).tolist()\n",
    "        t = tf.reduce_sum(attention_mask, 1).numpy().tolist()\n",
    "        cpt_loss = []\n",
    "        for i in range(len(input_ids)):\n",
    "            total = t[i]\n",
    "            num_masked = len(rationale_index[i])\n",
    "            cpt_loss.append(max((num_masked/total)-self.top_percentage,0+1e-12))\n",
    "        # Compactness\n",
    "        cpt_loss = torch.Tensor(cpt_loss).to(device)\n",
    "\n",
    "        # logical fluency\n",
    "        one_hot_o = rationale_weights.tolist()\n",
    "        new_tokens = []\n",
    "        for i in range(len(one_hot_o)):\n",
    "            for j in range(self.lf_num):\n",
    "                new_order = newset(rationale_index[i],one_hot_o[i],self.lf_seed,self.lf_num)[j]\n",
    "                # print(new_order)\n",
    "            new_tokens.append([input_ids[i].numpy()[k] for k in new_order])\n",
    "        new_tokens = torch.Tensor(new_tokens).to(device)\n",
    "        lf_cls_output = self.classify(\n",
    "            input_ids=new_tokens,\n",
    "            mask_weights=new_tokens.clone().fill_(1),\n",
    "            attention_mask=attention_mask,\n",
    "            # token_type_ids=token_type_ids,\n",
    "            # position_ids=position_ids,\n",
    "            labels=labels)\n",
    "\n",
    "        full_cls_output = self.classify(\n",
    "            input_ids=torch.Tensor(input_ids.numpy()),\n",
    "            mask_weights=torch.Tensor(input_ids.numpy()).clone().fill_(1),\n",
    "            attention_mask=attention_mask,\n",
    "            # token_type_ids=token_type_ids,\n",
    "            # position_ids=position_ids,\n",
    "            labels=labels)\n",
    "        # logical fluency\n",
    "        l_loss = lf_cls_output.loss if labels is not None else 0\n",
    "        f_loss = full_cls_output.loss if labels is not None else 0\n",
    "        lf_loss= []\n",
    "        for i in range(len(l_loss)):\n",
    "          lf_loss.append(max((l_loss[i]-f_loss[i]),0+1e-12))\n",
    "        lf_loss = torch.Tensor(lf_loss).to(device)\n",
    "\n",
    "        # Add loss components.\n",
    "        loss = ( suffiency_loss +\n",
    "                self.config.cph_weight * cph_loss +\n",
    "                self.config.cpt_weight * cpt_loss +\n",
    "                self.config.lf_weight * lf_loss) \n",
    "        loss = loss.mean()\n",
    "        loss.requires_grad = True\n",
    "\n",
    "        # print(masked_cls_output.logits.shape)\n",
    "        # print(mask_output.rationale.shape)\n",
    "        # print(labels.shape)\n",
    "        # print(mask_labels)\n",
    "        # print(mask_labels.shape)\n",
    "        # print(input_ids.shape)\n",
    "        return TokenTaggingRationaleOutput(\n",
    "            loss=loss,\n",
    "            # full_logits=full_cls_output.logits,\n",
    "            masked_logits=masked_cls_output.logits,\n",
    "            rationale=mask_output.rationale,\n",
    "            labels=labels,\n",
    "            mask_labels=mask_labels,\n",
    "            # is_unsupervised=is_unsupervised,\n",
    "            input_ids=input_ids,\n",
    "            s_loss=suffiency_loss.mean(),\n",
    "            cph_loss=cph_loss.mean(),\n",
    "            cpt_loss=cpt_loss.mean(),\n",
    "            lf_loss=lf_loss.mean(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca0cfcab-ba67-4165-ae41-9f26e91d0abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenRationale: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForTokenRationale from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenRationale from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenRationale were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.distilbert.transformer.layer.0.attention.v_lin.weight', 'classifier.distilbert.transformer.layer.0.attention.q_lin.weight', 'classifier.distilbert.transformer.layer.3.attention.k_lin.bias', 'classifier.distilbert.transformer.layer.3.attention.q_lin.weight', 'classifier.distilbert.transformer.layer.0.attention.out_lin.weight', 'classifier.distilbert.transformer.layer.4.attention.v_lin.bias', 'classifier.distilbert.embeddings.LayerNorm.weight', 'classifier.distilbert.transformer.layer.5.ffn.lin1.weight', 'classifier.distilbert.transformer.layer.4.attention.k_lin.weight', 'classifier.distilbert.transformer.layer.4.output_layer_norm.weight', 'classifier.distilbert.transformer.layer.3.output_layer_norm.bias', 'classifier.distilbert.transformer.layer.1.attention.q_lin.weight', 'classifier.distilbert.transformer.layer.5.output_layer_norm.bias', 'classifier.distilbert.transformer.layer.2.ffn.lin1.weight', 'classifier.distilbert.transformer.layer.2.attention.q_lin.bias', 'classifier.distilbert.transformer.layer.3.ffn.lin1.bias', 'classifier.distilbert.transformer.layer.2.output_layer_norm.bias', 'classifier.distilbert.transformer.layer.2.output_layer_norm.weight', 'classifier.distilbert.transformer.layer.2.attention.k_lin.bias', 'classifier.classifier.bias', 'classifier.distilbert.transformer.layer.5.attention.out_lin.weight', 'classifier.distilbert.transformer.layer.5.sa_layer_norm.weight', 'classifier.distilbert.transformer.layer.5.ffn.lin2.weight', 'classifier.distilbert.transformer.layer.4.sa_layer_norm.weight', 'classifier.distilbert.transformer.layer.1.attention.k_lin.weight', 'classifier.distilbert.transformer.layer.4.attention.out_lin.weight', 'classifier.distilbert.transformer.layer.0.attention.q_lin.bias', 'classifier.distilbert.transformer.layer.1.output_layer_norm.bias', 'classifier.distilbert.transformer.layer.2.sa_layer_norm.weight', 'classifier.distilbert.transformer.layer.4.attention.q_lin.bias', 'classifier.distilbert.transformer.layer.1.ffn.lin2.weight', 'classifier.distilbert.transformer.layer.0.attention.k_lin.bias', 'classifier.distilbert.transformer.layer.3.ffn.lin2.bias', 'classifier.distilbert.transformer.layer.5.attention.out_lin.bias', 'classifier.distilbert.embeddings.word_embeddings.weight', 'classifier.distilbert.transformer.layer.5.sa_layer_norm.bias', 'classifier.distilbert.transformer.layer.1.attention.out_lin.weight', 'classifier.distilbert.transformer.layer.1.output_layer_norm.weight', 'classifier.distilbert.transformer.layer.1.attention.v_lin.bias', 'classifier.distilbert.transformer.layer.1.sa_layer_norm.bias', 'classifier.distilbert.transformer.layer.0.ffn.lin1.weight', 'classifier.distilbert.transformer.layer.1.attention.k_lin.bias', 'classifier.distilbert.transformer.layer.4.ffn.lin1.weight', 'classifier.distilbert.transformer.layer.0.ffn.lin2.weight', 'classifier.distilbert.transformer.layer.4.sa_layer_norm.bias', 'classifier.distilbert.transformer.layer.0.attention.v_lin.bias', 'classifier.distilbert.transformer.layer.4.attention.out_lin.bias', 'classifier.distilbert.transformer.layer.5.attention.v_lin.bias', 'classifier.distilbert.transformer.layer.2.attention.out_lin.weight', 'classifier.distilbert.transformer.layer.4.ffn.lin2.weight', 'classifier.distilbert.transformer.layer.2.attention.v_lin.bias', 'classifier.distilbert.transformer.layer.0.ffn.lin1.bias', 'classifier.distilbert.transformer.layer.3.output_layer_norm.weight', 'classifier.distilbert.transformer.layer.1.sa_layer_norm.weight', 'classifier.distilbert.transformer.layer.4.attention.v_lin.weight', 'classifier.pre_classifier.bias', 'classifier.distilbert.transformer.layer.2.attention.q_lin.weight', 'classifier.distilbert.transformer.layer.1.attention.q_lin.bias', 'classifier.distilbert.transformer.layer.0.output_layer_norm.bias', 'classifier.distilbert.transformer.layer.5.attention.k_lin.bias', 'classifier.distilbert.transformer.layer.2.ffn.lin2.weight', 'classifier.pre_classifier.weight', 'classifier.distilbert.transformer.layer.1.ffn.lin1.weight', 'classifier.distilbert.transformer.layer.5.attention.q_lin.weight', 'classifier.distilbert.transformer.layer.5.ffn.lin2.bias', 'classifier.distilbert.transformer.layer.1.ffn.lin2.bias', 'classifier.distilbert.transformer.layer.3.attention.k_lin.weight', 'classifier.distilbert.embeddings.position_embeddings.weight', 'classifier.distilbert.transformer.layer.4.attention.q_lin.weight', 'classifier.distilbert.transformer.layer.3.attention.v_lin.bias', 'classifier.distilbert.transformer.layer.2.sa_layer_norm.bias', 'classifier.distilbert.transformer.layer.4.ffn.lin2.bias', 'classifier.classifier.weight', 'classifier.distilbert.transformer.layer.3.ffn.lin2.weight', 'classifier.distilbert.transformer.layer.5.attention.v_lin.weight', 'classifier.distilbert.transformer.layer.0.attention.k_lin.weight', 'classifier.distilbert.transformer.layer.0.ffn.lin2.bias', 'classifier.distilbert.transformer.layer.2.ffn.lin1.bias', 'classifier.distilbert.transformer.layer.5.output_layer_norm.weight', 'classifier.distilbert.transformer.layer.0.sa_layer_norm.weight', 'classifier.distilbert.transformer.layer.1.ffn.lin1.bias', 'classifier.distilbert.transformer.layer.3.attention.v_lin.weight', 'classifier.distilbert.transformer.layer.2.attention.v_lin.weight', 'classifier.distilbert.transformer.layer.5.ffn.lin1.bias', 'classifier.distilbert.transformer.layer.3.ffn.lin1.weight', 'classifier.distilbert.transformer.layer.2.ffn.lin2.bias', 'classifier.distilbert.transformer.layer.0.attention.out_lin.bias', 'classifier.distilbert.transformer.layer.1.attention.v_lin.weight', 'classifier.distilbert.transformer.layer.3.attention.out_lin.weight', 'classifier.distilbert.transformer.layer.3.attention.out_lin.bias', 'classifier.distilbert.transformer.layer.4.output_layer_norm.bias', 'classifier.distilbert.transformer.layer.3.sa_layer_norm.weight', 'classifier.distilbert.transformer.layer.3.sa_layer_norm.bias', 'classifier.distilbert.transformer.layer.4.attention.k_lin.bias', 'classifier.distilbert.transformer.layer.3.attention.q_lin.bias', 'classifier.distilbert.transformer.layer.4.ffn.lin1.bias', 'classifier.distilbert.transformer.layer.2.attention.k_lin.weight', 'classifier.distilbert.transformer.layer.2.attention.out_lin.bias', 'classifier.distilbert.transformer.layer.1.attention.out_lin.bias', 'classifier.distilbert.transformer.layer.5.attention.q_lin.bias', 'classifier.distilbert.transformer.layer.0.output_layer_norm.weight', 'classifier.distilbert.transformer.layer.0.sa_layer_norm.bias', 'classifier.distilbert.embeddings.LayerNorm.bias', 'classifier.distilbert.transformer.layer.5.attention.k_lin.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "... MODEL OUTPUT SHAPE : torch.Size([1, 2]) \n",
      "... MODEL OUTPUT LOSS  : 0.6406993269920349 \n",
      "\n",
      "\n",
      "\n",
      "... ADAMW OPTIMIZER OBJECT : AdamW (\n",
      "Parameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    correct_bias: True\n",
      "    eps: 1e-06\n",
      "    lr: 5e-05\n",
      "    weight_decay: 0.0\n",
      ") \n",
      "\n",
      "\n",
      "\n",
      "... LEARNING RATE SCHEDULER OBJECT   : <torch.optim.lr_scheduler.LambdaLR object at 0x7fa39c760820> \n",
      "... LEARNING RATE SCHEDULE # OF STEPS : 180 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mistgpu/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "# Instantiate the model using a checkpoint and define the number of label categories\n",
    "model = BertForTokenRationale.from_pretrained(checkpoint, num_labels=2)\n",
    "\n",
    "# Test the model\n",
    "outputs = model(**batch)\n",
    "print(f\"\\n\\n\\n... MODEL OUTPUT SHAPE : {outputs.masked_logits.shape} \")\n",
    "print(f\"... MODEL OUTPUT LOSS  : {outputs.loss} \")\n",
    "\n",
    "# Instantiate our optimizer using the defaults\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "print(f\"\\n\\n\\n... ADAMW OPTIMIZER OBJECT : {optimizer} \")\n",
    "\n",
    "# Instantiate our learning rate scheduler using defaults\n",
    "num_epochs = 5\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "print(f\"\\n\\n\\n... LEARNING RATE SCHEDULER OBJECT   : {lr_scheduler} \")\n",
    "print(f\"... LEARNING RATE SCHEDULE # OF STEPS : {num_training_steps} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88ace910-3985-47b7-aeba-0e97e40fe45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "... TRAINING WILL OCCUR USING cuda ...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a37f6e4fd087439f8b3acb2e5bdce94d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============更新之后===========\n",
      "25.684633135795593\n",
      "0.7134620315498776\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    correct_bias: True\n",
      "    eps: 1e-06\n",
      "    initial_lr: 5e-05\n",
      "    lr: 4e-05\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "===\n",
      "=============更新之后===========\n",
      "24.591809034347534\n",
      "0.6831058065096537\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    correct_bias: True\n",
      "    eps: 1e-06\n",
      "    initial_lr: 5e-05\n",
      "    lr: 3e-05\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "===\n",
      "=============更新之后===========\n",
      "24.801637142896652\n",
      "0.6889343650804626\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    correct_bias: True\n",
      "    eps: 1e-06\n",
      "    initial_lr: 5e-05\n",
      "    lr: 2e-05\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "===\n",
      "=============更新之后===========\n",
      "25.375915974378586\n",
      "0.7048865548438497\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    correct_bias: True\n",
      "    eps: 1e-06\n",
      "    initial_lr: 5e-05\n",
      "    lr: 1e-05\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "===\n",
      "=============更新之后===========\n",
      "24.74106204509735\n",
      "0.6872517234749265\n",
      "AdamW (\n",
      "Parameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    correct_bias: True\n",
      "    eps: 1e-06\n",
      "    initial_lr: 5e-05\n",
      "    lr: 0.0\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "===\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# If GPU available ensure training occurs on it... otherwise fallback to CPU\n",
    "#      - Get device\n",
    "#      - Push model to device (CPU or GPU)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Check which device we are using\n",
    "print(f\"\\n\\n\\n... TRAINING WILL OCCUR USING {device} ...\\n\")\n",
    "\n",
    "# Define the progress bar for training based on # of steps\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# --------------------- TRAINING LOOP ---------------------\n",
    "# ----------------------------------------------------------\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    loss_s = []\n",
    "    for batch in train_dataloader:\n",
    "        # batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        # for name, parms in model.named_parameters():\n",
    "        #         print('-->name:', name)\n",
    "        #         # print('-->para:', parms)\n",
    "        #         print('-->grad_requirs:',parms.requires_grad)\n",
    "        #         print('-->grad_value:',parms.grad)\n",
    "        #         print(\"===\")\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "        loss_s.append(loss.tolist())\n",
    "    # print(loss_s)\n",
    "    print(\"=============更新之后===========\")\n",
    "    print(sum(loss_s))\n",
    "    print(np.mean(loss_s))\n",
    "    \n",
    "    # for name, parms in model.named_parameters():\t\n",
    "    #     print('-->name:', name)\n",
    "    #     # print('-->para:', parms)\n",
    "    #     print('-->grad_requirs:',parms.requires_grad)\n",
    "    #     print('-->grad_value:',parms.grad)\n",
    "    print(optimizer)\n",
    "    print(\"===\")\n",
    "\n",
    "# ----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91099178-e2a7-4222-973d-b20430e19527",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84edd7e2-b2e8-4574-9004-4955c9e729fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
