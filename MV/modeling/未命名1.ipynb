{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de54119-a7b0-4ca7-b50a-f2914e46ec99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "732675ba-0891-4e08-a504-afefa67f3424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24a0ddbf-e54b-4754-8362-a4e86af697c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/mist/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df65a6307b914d61bfde3d509db21794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Download and cache the dataset\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79a090f6-4c0c-4857-8b21-d4799e5b5e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_dataset = raw_datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4c5ec28-4834-4c40-9056-7863df9a61a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c35bc58-88e8-407e-a659-40b3bcbdaf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenized_sentences_1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"])\n",
    "tokenized_sentences_2 = tokenizer(raw_datasets[\"train\"][\"sentence2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3180efa-9c3f-4147-b893-dd6521b8a4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/mist/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-6fd6978f4d0e1f7f.arrow\n",
      "Loading cached processed dataset at /home/mist/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-e21445cecc3a2248.arrow\n",
      "Loading cached processed dataset at /home/mist/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-74cc38742f918009.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    \"\"\"\n",
    "    This function takes a dictionary (like the items of our dataset) and returns a new dictionary \n",
    "    with the keys input_ids, attention_mask, and token_type_ids. \n",
    "        - Note that it also works if the example dictionary contains several samples \n",
    "          (each key as a list of sentences) since the tokenizer works on lists \n",
    "          of pairs of sentences, as seen before. \n",
    "    \n",
    "    This will allow us to use the option batched=True in our call to map(), \n",
    "    which will greatly speed up the tokenization. \n",
    "        - The tokenizer is backed by a tokenizer written in Rust from the \n",
    "          🤗 Tokenizers library. \n",
    "        - This tokenizer can be very fast, but only if we give it lots of \n",
    "          inputs at once.\n",
    "    \n",
    "    Note that we’ve left the padding argument out in our tokenization function for now. \n",
    "        - This is because padding all the samples to the maximum length is not efficient\n",
    "        - It’s better to pad the samples when we’re building a batch, as then we only need to \n",
    "          pad to the maximum length in that batch, and not the maximum length in the entire dataset. \n",
    "        - This can save a lot of time and processing power when the inputs have variable lengths.\n",
    "    \n",
    "    Args:\n",
    "        example (Dict): A dictionary containing the items of the dataset.\n",
    "    \n",
    "    Returns:\n",
    "        A new dictionary with the keys input_ids, attention_mask, and token_type_ids.\n",
    "    \n",
    "    \"\"\"\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
    "\n",
    "# Here is how we apply the tokenization function on all our datasets at once. \n",
    "# \n",
    "# We’re using batched=True in our call to map so the function is applied to \n",
    "# multiple elements of our dataset at once, and not on each element separately. \n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee8fcdf2-63df-4f19-8860-aeb7940631ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Remove the columns corresponding to values the model does not expect\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
    "\n",
    "# Rename the column label to labels\n",
    "#      - Because the model expects the argument to be named `labels`\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "\n",
    "# Set the format of the datasets so they return PyTorch tensors\n",
    "tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0599860b-0ad2-44c0-8b9a-4231a77097b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "l_len = len(tokenized_datasets[\"train\"])\n",
    "random.seed(1234)\n",
    "line_r = [random.randint(0,l_len-1) for _ in range(int(l_len*0.01))]\n",
    "tokenized_datasets[\"train\"] = [tokenized_datasets[\"train\"][i] for i in line_r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0c4e3bc-19cb-4ad5-bb0c-03c1621ea888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "l_len = len(tokenized_datasets[\"validation\"])\n",
    "random.seed(1234)\n",
    "line_r = [random.randint(0,l_len-1) for _ in range(int(l_len*0.01))]\n",
    "tokenized_datasets[\"validation\"] = [tokenized_datasets[\"validation\"][i] for i in line_r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab9ce2e1-ad2f-4610-aca7-cbec8f627f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bde5d6a2-0234-4b2b-8d25-e5ccafaad126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "from typing import Optional\n",
    "from sklearn import metrics\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import tensorflow as tf\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "from transformers.file_utils import ModelOutput\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    BertModel,\n",
    "    BertPreTrainedModel,\n",
    "    )\n",
    "\n",
    "from connected_shapley import explain_shapley, construct_positions_connectedshapley\n",
    "from continuity1 import  onehat_pre,continuity,rl_cd_ss\n",
    "from logical_fluency import newset\n",
    "\n",
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "   'bert-base-uncased',\n",
    "    num_labels=2,\n",
    "\n",
    ")\n",
    "\n",
    "def compute_metrics_fn(eval_prediction, tokenizer=None, return_text=False):\n",
    "    # full_logits = eval_prediction.predictions[0]\n",
    "    masked_logits = eval_prediction.predictions[0]\n",
    "    mask = eval_prediction.predictions[1]\n",
    "    labels = eval_prediction.predictions[2]\n",
    "    mask_labels = eval_prediction.predictions[3]\n",
    "    mask_labels = np.maximum(mask_labels, 0)\n",
    "\n",
    "    s_loss = eval_prediction.predictions[5]\n",
    "    cph_loss = eval_prediction.predictions[6]\n",
    "    cpt_loss = eval_prediction.predictions[7]\n",
    "    lf_loss = eval_prediction.predictions[8]\n",
    "\n",
    "    # full_preds = np.argmax(full_logits, axis=1)\n",
    "    masked_preds = np.argmax(masked_logits, axis=1)\n",
    "    is_masked = np.greater(mask, 0.5).astype(np.int)\n",
    "\n",
    "    results = {}\n",
    "    # results[\"full_acc\"] = metrics.accuracy_score(labels, full_preds)\n",
    "    results[\"masked_acc\"] = metrics.accuracy_score(labels, masked_preds)\n",
    "    results[\"mask_f1\"] = metrics.f1_score(mask_labels, is_masked, average=\"micro\", zero_division=1)\n",
    "    results[\"mask_recall\"] = metrics.recall_score(mask_labels, is_masked, average=\"micro\", zero_division=1)\n",
    "    results[\"mask_precision\"] = metrics.precision_score(mask_labels, is_masked, average=\"micro\", zero_division=1)\n",
    "\n",
    "    results[\"suffiency_loss\"] = s_loss.mean()\n",
    "    results[\"comprehensiveness_loss\"] = cph_loss.mean()\n",
    "    results[\"compactness_loss\"] = cpt_loss.mean()\n",
    "    results[\"logical_fluence_loss\"] = lf_loss.mean()\n",
    "\n",
    "    if return_text:\n",
    "        input_ids = eval_prediction.predictions[4]\n",
    "        examples = []\n",
    "        for i in range(len(input_ids)):\n",
    "            import pdb; pdb.set_trace()\n",
    "            row = tokenizer.convert_ids_to_tokens(input_ids[i])\n",
    "            original = []\n",
    "            masked = []\n",
    "            for j in range(len(row)):\n",
    "                if row[j] in [\"[CLS]\", \"[SEP]\", \"<pad>\"]:\n",
    "                    continue\n",
    "                original.append(row[j])\n",
    "                if is_masked[i][j]:\n",
    "                    masked.append(\"▁<mask>\")\n",
    "                else:\n",
    "                    masked.append(row[j])\n",
    "            original = tokenizer.convert_tokens_to_string(original)\n",
    "            masked = tokenizer.convert_tokens_to_string(masked)\n",
    "            combined = f\"{original} OLD: {masked}\"\n",
    "            examples.append(combined)\n",
    "        results[\"text\"] = examples\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "class MaskOutput(ModelOutput):\n",
    "    rationale: torch.FloatTensor\n",
    "    noise: torch.FloatTensor\n",
    "    r_ind: torch.FloatTensor\n",
    "\n",
    "\n",
    "\n",
    "class ClassifyOutput(ModelOutput):\n",
    "    logits: torch.FloatTensor\n",
    "    loss: torch.FloatTensor\n",
    "\n",
    "\n",
    "class TokenTaggingRationaleOutput(ModelOutput):\n",
    "    loss: torch.FloatTensor\n",
    "    # full_logits: torch.FloatTensor\n",
    "    masked_logits: torch.FloatTensor\n",
    "    mask: torch.FloatTensor\n",
    "    labels: Optional[torch.FloatTensor] = None\n",
    "    mask_labels: Optional[torch.FloatTensor] = None\n",
    "    # is_unsupervised: Optional[torch.FloatTensor] = None\n",
    "    input_ids: Optional[torch.LongTensor] = None\n",
    "    s_loss: torch.FloatTensor = None\n",
    "    cph_loss:torch.FloatTensor = None\n",
    "    cpt_loss:torch.FloatTensor = None\n",
    "    lf_loss:torch.FloatTensor = None\n",
    "\n",
    "\n",
    "class BertForTokenRationale(BertPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertForTokenRationale, self).__init__(config)\n",
    "        self.config = config\n",
    "        self.num_labels = 2\n",
    "        # self.nei_index = config.nei_index\n",
    "        self.mask_token_id = tokenizer.mask_token_id\n",
    "        self.bert = BertModel(config).to(device)\n",
    "        # self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        # # nn.Dropout: 为了防止或减轻过拟合而使用的函数\n",
    "        # self.masker = nn.Linear(config.hidden_size, 2)\n",
    "        # # 是用来设置网络中的全连接层的，而在全连接层中的输入与输出都是二维张量，一般形状为[batch_size, size]\n",
    "\n",
    "\n",
    "        # self.sufficiency_weight = config.sufficiency_weight # sufficiency 好像不需要weight\n",
    "        self.config.cph_weight = 0.3\n",
    "        self.config.cpt_weight = 0.2\n",
    "        self.config.lf_weight = 0.2\n",
    "        # self.batch_size = config.per_device_train_batch_size\n",
    "\n",
    "        # self.train_size=config.per_device_train_batch_size\n",
    "        # self.max_seq_length = config.max_seq_length\n",
    "\n",
    "        # shapley\n",
    "        # self.max_order = config.max_order\n",
    "        self.num_neighbors = 2\n",
    "        self.top_percentage = 0.2\n",
    "        self.con_count =3\n",
    "\n",
    "        self.cph_margin =0.05\n",
    "        self.lf_seed=1234\n",
    "        self.lf_num = 1\n",
    "\n",
    "\n",
    "        self.init_weights()\n",
    "        self.classifier = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english').to(device)\n",
    "        for p in self.classifier.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def mask(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        # token_type_ids=None,\n",
    "        # position_ids=None,\n",
    "        evidence_mask=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        # print(type(attention_mask))\n",
    "        # print(attention_mask)\n",
    "        # d = tf.reduce_sum(attention_mask, 1).numpy().tolist()\n",
    "        attention_mask.tolist().sum(axis=1)\n",
    "        rationale_weights = []\n",
    "        noise_weights = []\n",
    "        r_d = []\n",
    "        for i in range(len(input_ids)):\n",
    "            positions_dict, key_to_idx, positions, coefficients, unique_inverse = construct_positions_connectedshapley(int(d[i]),k=2)\n",
    "\n",
    "\n",
    "            mask_weights = torch.Tensor(positions).unsqueeze(0).to(device)\n",
    "            mask_ids = input_ids[i][:d[i]].clone().fill_(self.mask_token_id).to(device)\n",
    "            input_id = input_ids[i][:d[i]].to(device)\n",
    "            mix_inputs = (1-mask_weights) * mask_ids +  mask_weights * input_id\n",
    "            mix_inputs = mix_inputs.long().squeeze().to(device)\n",
    "            \n",
    "            \n",
    "            score = explain_shapley(self.bert,int(d[i]), self.num_neighbors, key_to_idx,mix_inputs, input_id.long(),\n",
    "                                    coefficients, unique_inverse)\n",
    "\n",
    "            # score = torch.tensor(np.array(score))\n",
    "            r_len = math.ceil(int(d[i])*self.top_percentage)\n",
    "            # score_ind_order = torch.sort(score).indices\n",
    "            # score_val_order = torch.sort(score).values\n",
    "\n",
    "            one_hot = [onehat_pre(score,r_len)]\n",
    "            ra_can = continuity(score, one_hot, self.con_count)\n",
    "            s_d_r = rl_cd_ss(ra_can,score)\n",
    "\n",
    "            max_sdr_ind = s_d_r.index(max(s_d_r))\n",
    "\n",
    "\n",
    "            score_ind_order_d = torch.sort(torch.Tensor(np.array(score)),descending=False).indices[:r_len]\n",
    "\n",
    "            nw = torch.Tensor([1 if i in score_ind_order_d else 0 for i in range(int(d[i]))]).to(device)\n",
    "            nw = pad_sequence([nw,input_id]).T[0].tolist()\n",
    "\n",
    "            noise_weights.append(nw)\n",
    "\n",
    "\n",
    "            rw = torch.tensor(ra_can[max_sdr_ind]).to(device)\n",
    "            rw = pad_sequence([rw,input_id]).T[0].tolist()\n",
    "            rationale_weights.append(rw)\n",
    "            r_d.append([i for i, x in enumerate(ra_can[max_sdr_ind]) if x == 1])\n",
    "\n",
    "        rationale_weights = torch.Tensor(rationale_weights)\n",
    "        noise_weights = torch.Tensor(noise_weights)\n",
    "\n",
    "\n",
    "        rationale_weights = F.gumbel_softmax(rationale_weights, tau=0.1)\n",
    "        noise_weights = F.gumbel_softmax(noise_weights, tau=0.1)\n",
    "\n",
    "        return MaskOutput(\n",
    "            rationale=rationale_weights,\n",
    "            noise = noise_weights,\n",
    "            r_ind=r_d,\n",
    "\n",
    "        )\n",
    "\n",
    "    def classify(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        mask_weights=None,\n",
    "        attention_mask=None,\n",
    "        # token_type_ids=None,\n",
    "        # position_ids=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        def embed(ids):\n",
    "            embeds = self.classifier.get_input_embeddings()(ids)\n",
    "            return embeds\n",
    "\n",
    "        # Embed inputs.\n",
    "        input_ids = input_ids.long().to(device)\n",
    "        input_embeds = embed(input_ids)\n",
    "\n",
    "        # Targeted mask.\n",
    "        mask_ids = input_ids.clone().fill_(self.mask_token_id).to(device)\n",
    "        mask_embeds = embed(mask_ids).to(device)\n",
    "        mask_weights = mask_weights.unsqueeze(-1).to(device)\n",
    "\n",
    "        # Mix embeddings.\n",
    "        # 我们直接用0mask还是像这样用 additive embedding呢\n",
    "        mix_embeds = (1-mask_weights) * mask_embeds +  mask_weights * input_embeds\n",
    "        \n",
    "        # print(type(mix_embeds))\n",
    "        # print(type(attention_mask))\n",
    "        # attention_mask = torch.Tensor(attention_mask.numpy()).long().to(device)\n",
    "        # labels= torch.Tensor(labels.numpy()).long().to(device)\n",
    "        # Run model.\n",
    "        outputs = self.classifier(\n",
    "            inputs_embeds=mix_embeds,\n",
    "            # attention_mask=attention_mask,\n",
    "            # token_type_ids=token_type_ids,\n",
    "            # position_ids=position_ids,\n",
    "            return_dict=True\n",
    "        )\n",
    "        logits = outputs.logits\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return ClassifyOutput(\n",
    "            logits=logits,\n",
    "            loss=loss,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        # token_type_ids=None,\n",
    "        # position_ids=None,\n",
    "        evidence_mask=None,\n",
    "        labels=None,\n",
    "        mask_labels=None,\n",
    "        # is_unsupervised=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        # Encode the inputs and predict token-level masking scores.\n",
    "        mask_output = self.mask(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            # token_type_ids=token_type_ids,\n",
    "            # position_ids=position_ids,\n",
    "            evidence_mask=evidence_mask,\n",
    "            labels=None)\n",
    "        rationale_weights = mask_output.rationale\n",
    "        noise_weights = mask_output.noise\n",
    "        rationale_index = mask_output.r_ind\n",
    "\n",
    "\n",
    "        # Get the output with targeted masking.\n",
    "        masked_cls_output = self.classify(\n",
    "            input_ids=input_ids,\n",
    "            mask_weights=rationale_weights,\n",
    "            attention_mask=attention_mask,\n",
    "            # token_type_ids=token_type_ids,\n",
    "            # position_ids=position_ids,\n",
    "            labels=labels)\n",
    "        # Suffiency\n",
    "        suffiency_loss = masked_cls_output.loss if labels is not None else 0\n",
    "\n",
    "        noise_cls_output = self.classify(\n",
    "            input_ids=input_ids,\n",
    "            mask_weights=noise_weights,\n",
    "            attention_mask=attention_mask,\n",
    "            # token_type_ids=token_type_ids,\n",
    "            # position_ids=position_ids,\n",
    "            labels=labels)\n",
    "        # Noise\n",
    "        noise_loss = noise_cls_output.loss if labels is not None else 0\n",
    "\n",
    "        # Comprehensive\n",
    "        cph_loss = []\n",
    "        for i in range(len(suffiency_loss)):\n",
    "            cph_loss.append(max(suffiency_loss[i]-noise_loss[i]+self.cph_margin,0+1e-12))\n",
    "        cph_loss = torch.Tensor(cph_loss).to(device)\n",
    "\n",
    "        # Compute (soft) number of masked tokens\n",
    "        mask_w = mask_output.rationale.sum(dim=1).tolist()\n",
    "        t = attention_mask.sum(dim=1).tolist()\n",
    "        cpt_loss = []\n",
    "        for i in range(len(input_ids)):\n",
    "            total = t[i]\n",
    "            num_masked = len(rationale_index[i])\n",
    "            cpt_loss.append(max((num_masked/total)-self.top_percentage,0+1e-12))\n",
    "        # Compactness\n",
    "        cpt_loss = torch.Tensor(cpt_loss).to(device)\n",
    "\n",
    "        # logical fluency\n",
    "        one_hot_o = rationale_weights.tolist()\n",
    "        new_tokens = []\n",
    "        for i in range(len(one_hot_o)):\n",
    "            for j in range(self.lf_num):\n",
    "                new_order = newset(rationale_index[i],one_hot_o[i],self.lf_seed,self.lf_num)[j]\n",
    "                # print(new_order)\n",
    "            new_tokens.append([input_ids[i][k] for k in new_order])\n",
    "        new_tokens = torch.Tensor(new_tokens).to(device)\n",
    "        lf_cls_output = self.classify(\n",
    "            input_ids=new_tokens,\n",
    "            mask_weights=new_tokens.clone().fill_(1),\n",
    "            attention_mask=attention_mask,\n",
    "            # token_type_ids=token_type_ids,\n",
    "            # position_ids=position_ids,\n",
    "            labels=labels)\n",
    "\n",
    "        full_cls_output = self.classify(\n",
    "            input_ids=input_ids,\n",
    "            mask_weights=input_ids.clone().fill_(1),\n",
    "            attention_mask=attention_mask,\n",
    "            # token_type_ids=token_type_ids,\n",
    "            # position_ids=position_ids,\n",
    "            labels=labels)\n",
    "        # logical fluency\n",
    "        l_loss = lf_cls_output.loss if labels is not None else 0\n",
    "        f_loss = full_cls_output.loss if labels is not None else 0\n",
    "        lf_loss= []\n",
    "        for i in range(len(l_loss)):\n",
    "            lf_loss.append(max((l_loss[i]-f_loss[i]),0+1e-12))\n",
    "        lf_loss = torch.Tensor(lf_loss).to(device)\n",
    "\n",
    "        # Add loss components.\n",
    "        loss = ( suffiency_loss +\n",
    "                self.config.cph_weight * cph_loss +\n",
    "                self.config.cpt_weight * cpt_loss +\n",
    "                self.config.lf_weight * lf_loss) \n",
    "        loss = loss.mean()\n",
    "        loss.requires_grad = True\n",
    "\n",
    "        # print(masked_cls_output.logits.shape)\n",
    "        # print(mask_output.rationale.shape)\n",
    "        # print(labels.shape)\n",
    "        # print(mask_labels)\n",
    "        # print(mask_labels.shape)\n",
    "        # print(input_ids.shape)\n",
    "        return TokenTaggingRationaleOutput(\n",
    "            loss=loss,\n",
    "            # full_logits=full_cls_output.logits,\n",
    "            masked_logits=masked_cls_output.logits,\n",
    "            rationale=mask_output.rationale,\n",
    "            labels=labels,\n",
    "            mask_labels=mask_labels,\n",
    "            # is_unsupervised=is_unsupervised,\n",
    "            input_ids=input_ids,\n",
    "            s_loss=suffiency_loss.mean(),\n",
    "            cph_loss=cph_loss.mean(),\n",
    "            cpt_loss=cpt_loss.mean(),\n",
    "            lf_loss=lf_loss.mean(),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e452432-2369-4e1a-b258-9d56ddd9301e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    \"\"\" Wrapper function for computing metrics \n",
    "    \n",
    "    Args:\n",
    "        eval_preds: TBD\n",
    "        \n",
    "    Returns:\n",
    "        TBD\n",
    "    \"\"\"\n",
    "    metric = load_metric(\"glue\", \"mrpc\")\n",
    "    \n",
    "    if len(eval_preds)==3:\n",
    "        logits, labels, _ = eval_preds\n",
    "    else:\n",
    "        logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "904a4aea-d29f-4041-8d98-0af7acca35be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenRationale: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForTokenRationale from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenRationale from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenRationale were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.distilbert.transformer.layer.5.sa_layer_norm.bias', 'classifier.distilbert.transformer.layer.3.ffn.lin1.bias', 'classifier.distilbert.transformer.layer.0.ffn.lin1.weight', 'classifier.distilbert.transformer.layer.2.sa_layer_norm.weight', 'classifier.distilbert.embeddings.position_embeddings.weight', 'classifier.distilbert.transformer.layer.0.attention.out_lin.bias', 'classifier.distilbert.transformer.layer.3.attention.v_lin.weight', 'classifier.distilbert.transformer.layer.5.output_layer_norm.bias', 'classifier.distilbert.transformer.layer.5.ffn.lin2.bias', 'classifier.distilbert.transformer.layer.3.attention.q_lin.weight', 'classifier.distilbert.transformer.layer.4.output_layer_norm.bias', 'classifier.distilbert.transformer.layer.4.attention.q_lin.weight', 'classifier.distilbert.transformer.layer.0.attention.q_lin.weight', 'classifier.distilbert.transformer.layer.0.output_layer_norm.bias', 'classifier.distilbert.transformer.layer.0.attention.out_lin.weight', 'classifier.distilbert.transformer.layer.4.attention.k_lin.bias', 'classifier.distilbert.embeddings.LayerNorm.weight', 'classifier.distilbert.transformer.layer.0.ffn.lin2.bias', 'classifier.distilbert.transformer.layer.3.ffn.lin1.weight', 'classifier.distilbert.transformer.layer.5.sa_layer_norm.weight', 'classifier.distilbert.transformer.layer.2.ffn.lin2.weight', 'classifier.distilbert.transformer.layer.3.sa_layer_norm.bias', 'classifier.distilbert.transformer.layer.2.sa_layer_norm.bias', 'classifier.distilbert.transformer.layer.1.ffn.lin1.weight', 'classifier.distilbert.transformer.layer.4.attention.k_lin.weight', 'classifier.distilbert.transformer.layer.4.attention.out_lin.bias', 'classifier.distilbert.transformer.layer.1.sa_layer_norm.weight', 'classifier.distilbert.transformer.layer.5.attention.q_lin.bias', 'classifier.distilbert.transformer.layer.3.attention.q_lin.bias', 'classifier.distilbert.transformer.layer.1.ffn.lin1.bias', 'classifier.distilbert.transformer.layer.0.attention.k_lin.bias', 'classifier.distilbert.transformer.layer.2.attention.out_lin.bias', 'classifier.distilbert.transformer.layer.3.ffn.lin2.weight', 'classifier.distilbert.transformer.layer.3.attention.out_lin.bias', 'classifier.distilbert.transformer.layer.4.ffn.lin1.bias', 'classifier.distilbert.transformer.layer.3.attention.v_lin.bias', 'classifier.distilbert.transformer.layer.2.attention.q_lin.weight', 'classifier.distilbert.transformer.layer.3.output_layer_norm.weight', 'classifier.distilbert.transformer.layer.5.attention.out_lin.bias', 'classifier.distilbert.transformer.layer.0.attention.q_lin.bias', 'classifier.distilbert.transformer.layer.5.attention.out_lin.weight', 'classifier.distilbert.transformer.layer.5.attention.v_lin.weight', 'classifier.distilbert.transformer.layer.1.ffn.lin2.weight', 'classifier.distilbert.transformer.layer.4.sa_layer_norm.bias', 'classifier.distilbert.transformer.layer.2.attention.out_lin.weight', 'classifier.distilbert.transformer.layer.0.attention.v_lin.bias', 'classifier.distilbert.transformer.layer.1.attention.v_lin.weight', 'classifier.distilbert.transformer.layer.4.ffn.lin2.bias', 'classifier.distilbert.transformer.layer.4.output_layer_norm.weight', 'classifier.classifier.weight', 'classifier.distilbert.transformer.layer.5.ffn.lin2.weight', 'classifier.classifier.bias', 'classifier.distilbert.transformer.layer.2.attention.q_lin.bias', 'classifier.distilbert.transformer.layer.2.attention.v_lin.weight', 'classifier.distilbert.transformer.layer.5.ffn.lin1.bias', 'classifier.distilbert.transformer.layer.4.attention.v_lin.weight', 'classifier.distilbert.transformer.layer.3.sa_layer_norm.weight', 'classifier.distilbert.transformer.layer.0.ffn.lin1.bias', 'classifier.distilbert.transformer.layer.0.attention.k_lin.weight', 'classifier.distilbert.transformer.layer.1.attention.q_lin.bias', 'classifier.distilbert.transformer.layer.2.output_layer_norm.weight', 'classifier.distilbert.transformer.layer.2.ffn.lin1.weight', 'classifier.distilbert.transformer.layer.1.attention.q_lin.weight', 'classifier.distilbert.transformer.layer.2.attention.k_lin.bias', 'classifier.distilbert.transformer.layer.1.output_layer_norm.weight', 'classifier.distilbert.transformer.layer.0.attention.v_lin.weight', 'classifier.distilbert.transformer.layer.3.attention.k_lin.weight', 'classifier.distilbert.transformer.layer.3.ffn.lin2.bias', 'classifier.distilbert.embeddings.word_embeddings.weight', 'classifier.distilbert.transformer.layer.5.output_layer_norm.weight', 'classifier.distilbert.transformer.layer.4.ffn.lin1.weight', 'classifier.distilbert.transformer.layer.4.ffn.lin2.weight', 'classifier.distilbert.transformer.layer.2.attention.k_lin.weight', 'classifier.distilbert.transformer.layer.3.attention.out_lin.weight', 'classifier.distilbert.transformer.layer.4.attention.out_lin.weight', 'classifier.distilbert.transformer.layer.0.sa_layer_norm.bias', 'classifier.distilbert.transformer.layer.2.ffn.lin2.bias', 'classifier.distilbert.transformer.layer.1.attention.k_lin.bias', 'classifier.distilbert.transformer.layer.4.attention.v_lin.bias', 'classifier.distilbert.transformer.layer.1.attention.k_lin.weight', 'classifier.distilbert.transformer.layer.1.ffn.lin2.bias', 'classifier.distilbert.transformer.layer.5.attention.k_lin.weight', 'classifier.distilbert.transformer.layer.1.attention.out_lin.weight', 'classifier.distilbert.transformer.layer.5.ffn.lin1.weight', 'classifier.distilbert.transformer.layer.1.attention.out_lin.bias', 'classifier.pre_classifier.weight', 'classifier.pre_classifier.bias', 'classifier.distilbert.transformer.layer.2.output_layer_norm.bias', 'classifier.distilbert.embeddings.LayerNorm.bias', 'classifier.distilbert.transformer.layer.2.ffn.lin1.bias', 'classifier.distilbert.transformer.layer.1.sa_layer_norm.bias', 'classifier.distilbert.transformer.layer.3.attention.k_lin.bias', 'classifier.distilbert.transformer.layer.5.attention.q_lin.weight', 'classifier.distilbert.transformer.layer.5.attention.v_lin.bias', 'classifier.distilbert.transformer.layer.4.attention.q_lin.bias', 'classifier.distilbert.transformer.layer.1.output_layer_norm.bias', 'classifier.distilbert.transformer.layer.0.output_layer_norm.weight', 'classifier.distilbert.transformer.layer.5.attention.k_lin.bias', 'classifier.distilbert.transformer.layer.1.attention.v_lin.bias', 'classifier.distilbert.transformer.layer.2.attention.v_lin.bias', 'classifier.distilbert.transformer.layer.4.sa_layer_norm.weight', 'classifier.distilbert.transformer.layer.0.sa_layer_norm.weight', 'classifier.distilbert.transformer.layer.3.output_layer_norm.bias', 'classifier.distilbert.transformer.layer.0.ffn.lin2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/mistgpu/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 36\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 108\n",
      "The following columns in the training set don't have a corresponding argument in `BertForTokenRationale.forward` and have been ignored: token_type_ids. If token_type_ids are not expected by `BertForTokenRationale.forward`,  you can safely ignore this message.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'sum'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m BertForTokenRationale\u001b[38;5;241m.\u001b[39mfrom_pretrained(checkpoint, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     11\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     12\u001b[0m     model,\n\u001b[1;32m     13\u001b[0m     training_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     19\u001b[0m )\n\u001b[0;32m---> 20\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mistgpu/site-packages/transformers/trainer.py:1500\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1495\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1497\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1498\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1499\u001b[0m )\n\u001b[0;32m-> 1500\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mistgpu/site-packages/transformers/trainer.py:1742\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1740\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1741\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1742\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1745\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1746\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1748\u001b[0m ):\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/mistgpu/site-packages/transformers/trainer.py:2486\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2485\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2486\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2489\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m/mistgpu/site-packages/transformers/trainer.py:2518\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2516\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2517\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2518\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2519\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2520\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mBertForTokenRationale.forward\u001b[0;34m(self, input_ids, attention_mask, evidence_mask, labels, mask_labels, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    274\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    284\u001b[0m \n\u001b[1;32m    285\u001b[0m     \u001b[38;5;66;03m# Encode the inputs and predict token-level masking scores.\u001b[39;00m\n\u001b[0;32m--> 286\u001b[0m     mask_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# token_type_ids=token_type_ids,\u001b[39;49;00m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# position_ids=position_ids,\u001b[39;49;00m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevidence_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevidence_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m     rationale_weights \u001b[38;5;241m=\u001b[39m mask_output\u001b[38;5;241m.\u001b[39mrationale\n\u001b[1;32m    294\u001b[0m     noise_weights \u001b[38;5;241m=\u001b[39m mask_output\u001b[38;5;241m.\u001b[39mnoise\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mBertForTokenRationale.mask\u001b[0;34m(self, input_ids, attention_mask, evidence_mask, labels)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmask\u001b[39m(\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    155\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# print(attention_mask)\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# d = tf.reduce_sum(attention_mask, 1).numpy().tolist()\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m     \u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    166\u001b[0m     rationale_weights \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    167\u001b[0m     noise_weights \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'sum'"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "\n",
    "training_args = TrainingArguments(\"test-trainer\", \n",
    "                                  num_train_epochs=3,\n",
    "                                  per_device_train_batch_size=1,\n",
    "                                  per_device_eval_batch_size=1,\n",
    "                                  report_to=\"none\",\n",
    "                                  evaluation_strategy=\"epoch\")\n",
    "model = BertForTokenRationale.from_pretrained(checkpoint, num_labels=2)\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd74fdf9-c390-4bbf-a4b3-4884aba9fee8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
